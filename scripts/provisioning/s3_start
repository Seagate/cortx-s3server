#!/usr/bin/env python3
#
# Copyright (c) 2021 Seagate Technology LLC and/or its Affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#

import sys
import argparse
import traceback
import errno
import logging
import os
import shutil
import socket
import time
from logging import handlers
from s3confstore.cortx_s3_confstore import S3CortxConfStore

def main():
  """

  Main method takes care of calling
  a startup script of service given
  as a argument.

  """
  parser = argparse.ArgumentParser("S3server single entry command")
  parser.add_argument("--service", required=True)
  parser.add_argument("--index", required=False)
  parser.add_argument("-c", required=True)

  args = parser.parse_args()


  # global variables
  global s3_confkeys_store
  global base_config_path
  global logger

  try:
    if not args.c:
      raise Exception("provide confstore_url")
    confstore_url = args.c

    # Load confstore file
    s3_confkeys_store = S3CortxConfStore\
        (f'yaml:///opt/seagate/cortx/s3/mini-prov/s3_prov_config.yaml', \
          's3_index')
    prov_confkeys_store = S3CortxConfStore(f'{confstore_url}', \
          'prov_index')
    machine_id = s3_confkeys_store.get_machine_id()

    # Get Config entries
    config_key = s3_confkeys_store.get_config\
                     ("CONFIG>CONFSTORE_BASE_CONFIG_PATH")

    base_config_path = prov_confkeys_store.get_config(config_key)
    logdir_key = s3_confkeys_store.get_config\
                     ("CONFIG>CONFSTORE_BASE_LOG_PATH")
    base_logdir_path = prov_confkeys_store.get_config(logdir_key)

    #Logging path details
    s3_single_entry_log_directory = \
        os.path.join(base_logdir_path, "s3", machine_id, "s3deployment")
    s3_single_entry_log_file = \
        os.path.join(s3_single_entry_log_directory, "s3deployment.log")
    s3_single_entry_logger_name = \
        "s3deployment-logger-" + "[" + machine_id + "]"
    s3_single_entry_log_format = \
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    create_logger_directory(s3_single_entry_log_directory)
    logger = create_logger(s3_single_entry_log_file, s3_single_entry_logger_name, s3_single_entry_log_format)

    if args.service == 's3server':
      if not args.index:
        raise Exception("provide instance id")

      # get the s3 config file path from S3 Provisioner config file
      s3configfile = s3_confkeys_store.get_config("S3_CONFIG_FILE").replace("/opt/seagate/cortx", base_config_path)
      logger.info(f"s3configfile : {s3configfile}")
      # create symbolic link for this config file to be used by log rotation
      create_symbolic_link(s3configfile, s3_confkeys_store.get_config("S3_CONF_SYMLINK"))

      # configure s3server, s3m0trace, s3addb logrotate
      configure_log_rotate("S3_LOGROTATE_S3LOG", "/etc/cron.hourly/")
      configure_log_rotate("S3_LOGROTATE_M0TRACE", "/etc/cron.hourly/")
      configure_log_rotate("S3_LOGROTATE_ADDB", "/etc/cron.hourly/")
      configure_log_rotate("S3_LOGROTATE_AUDITLOG", "/etc/cron.daily/")

      # start crond
      os.system("/usr/sbin/crond start")

      #Sys config path
      sysconfig_path = os.path.join(base_config_path, "s3", "sysconfig",machine_id)

      fid_file = "s3server-" + args.index
      fid_path = os.path.join(sysconfig_path, fid_file)
      Fid = get_fid(fid_path)

      logger.info(f"sh /opt/seagate/cortx/s3/s3startsystem.sh -F {Fid} -P {fid_path} -C {s3configfile} -d")
      logger.info(f"Starting S3 Server with FID {Fid}...")
      i = 0
      while i < 10:
        # Start S3 Server
        os.system(f"sh /opt/seagate/cortx/s3/s3startsystem.sh -F {Fid} -P {fid_path} -C {s3configfile} -d")
        i += 1
        time.sleep(60)

      # Read the log directory path from s3config file
      s3_log_dir = read_config_value("S3_CONFIG_FILE", "yaml", "S3_SERVER_CONFIG>S3_LOG_DIR")
      logger.info(f"s3_log_dir : {s3_log_dir}")
      # Append FID directory and SERVER.INFO
      s3_log_file_path = os.path.join(s3_log_dir, fid_file, "SERVER.INFO")
      logger.info(f"s3_log_file_path : {s3_log_file_path}")

      # Print last few line of S3 server log file
      os.system(f"tail -f {s3_log_file_path}")
    elif args.service == 's3authserver':
      # Start S3 Auth Server
      logger.info(f"sh /opt/seagate/cortx/auth/startauth.sh {base_config_path}")
      logger.info(f"Starting S3 Auth Server...")
      os.system(f"sh /opt/seagate/cortx/auth/startauth.sh {base_config_path}")

      # Read the log directory path from s3 authserver config file
      s3_auth_server_log_dir = read_config_value("S3_AUTHSERVER_CONFIG_FILE", "properties", "logFilePath")
      logger.info(f"s3_auth_server_log_dir : {s3_auth_server_log_dir}")
      # Append app.log to directory path
      s3_auth_server_log_file_path = os.path.join(s3_auth_server_log_dir, "app.log")
      logger.info(f"s3_auth_server_log_file_path : {s3_auth_server_log_file_path}")

      # Print last few line of S3 authserver log  file
      os.system(f"tail -f {s3_auth_server_log_file_path}")
    elif args.service == 's3bgproducer':
      # Uncomment below two lines if you want return bgdelete schedule timings.
      # os.system("sed -i -e \"s,scheduler_schedule_interval:.*,scheduler_schedule_interval:\ 120,\" /etc/cortx/s3/s3backgrounddelete/config.yaml")
      # os.system("sed -i -e \"s,leak_processing_delay_in_mins:.*,leak_processing_delay_in_mins:\ 2,\" /etc/cortx/s3/s3backgrounddelete/config.yaml")
      # Start S3 Background Producer
      logger.info(f"sh /opt/seagate/cortx/s3/s3backgrounddelete/starts3backgroundproducer.sh")
      logger.info(f"Starting S3 Background delete Producer...")
      os.system("sh /opt/seagate/cortx/s3/s3backgrounddelete/starts3backgroundproducer.sh")

      # Read the log directory path from s3 bgdelete config file
      s3_bgdelete_producer_log_file_path = read_config_value("S3_BGDELETE_CONFIG_FILE", "yaml", "logconfig>scheduler_log_file")
      logger.info(f"s3_bgdelete_producer_log_file_path : {s3_bgdelete_producer_log_file_path}")

      # Print last few line of S3 bgdelete producer log  file
      os.system(f"tail -f {s3_bgdelete_producer_log_file_path}")
    elif args.service == 's3bgconsumer':
      # Start the Background Consumer
      logger.info(f"sh /opt/seagate/cortx/s3/s3backgrounddelete/starts3backgroundconsumer.sh")
      logger.info(f"Starting S3 Background delete Consumer...")
      os.system("sh /opt/seagate/cortx/s3/s3backgrounddelete/starts3backgroundconsumer.sh")

      # Read the log directory path from s3 bgdelete config file
      s3_bgdelete_consumer_log_file_path = read_config_value("S3_BGDELETE_CONFIG_FILE", "yaml", "logconfig>processor_log_file")
      logger.info(f"s3_bgdelete_consumer_log_file_path : {s3_bgdelete_consumer_log_file_path}")

      # Print last few line of S3 bgdelete consumer log  file
      os.system(f"tail -f {s3_bgdelete_consumer_log_file_path}")
    elif args.service == 'haproxy':
      haproxy_log_path = os.path.join(base_logdir_path, 's3', machine_id, 'haproxy/haproxy.log')
      logger.info(f"haproxy_log_path: {haproxy_log_path}")
      # create haproxy log directory path if does not exist
      os.makedirs(os.path.dirname(haproxy_log_path), exist_ok=True)
      # create empty haproxy log file
      with open(haproxy_log_path, 'w') as fp:
          pass
      # create symbolic link for this config file to be used by log rotation
      create_symbolic_link(haproxy_log_path,s3_confkeys_store.get_config("S3_HAPROXY_LOG_SYMLINK"))

      # configure haproxy logrotate using logrotate.d script
      update_haproxy_log_rotate_file("/etc/logrotate.d/haproxy", "/var/log/haproxy.log", f"{haproxy_log_path}")

      # start crond
      os.system("/usr/sbin/crond start")

      # Start haproxy
      logger.info(f"sh /opt/seagate/cortx/s3/bin/starthaproxy.sh {base_config_path} {haproxy_log_path}")
      logger.info(f"Starting HAProxy...")
      os.system(f"sh /opt/seagate/cortx/s3/bin/starthaproxy.sh {base_config_path} {haproxy_log_path}")

      # Print last few line of S3 HAProxy log  file
      os.system(f"tail -f {haproxy_log_path}")
    else:
      logger.error(f"Invalid argument {args.service}")
  except Exception as e:
    logger.error(f"{str(e)}")
    logger.error(f"{traceback.format_exc()}")
    return errno.EINVAL

  return 0

def read_config_value(config_file_path,
                      config_file_type,
                      key_to_read):
    # get the  config file path from S3 Provisioner config file
    configfile = s3_confkeys_store.get_config(config_file_path).replace("/opt/seagate/cortx", base_config_path)
    if os.path.isfile(f'{configfile}') == False:
      raise Exception(f'{configfile} file is not present')

    # load config file (example: s3configfileconfstore = confstore object to /etc/cortx/s3/conf/s3config.yaml)
    s3configfileconfstore = S3CortxConfStore(f'{config_file_type}://{configfile}', 'read_config_file_log_dir' + key_to_read)

    # read the given key from the config file
    config_value = s3configfileconfstore.get_config(key_to_read)

    return config_value

def create_logger(s3_single_entry_log_file, s3_single_entry_logger_name, s3_single_entry_log_format):
    """Create logger, file handler, console handler and formatter."""
    # create logger with "S3 deployment logger"
    logger = logging.getLogger(s3_single_entry_logger_name)
    logger.setLevel(logging.DEBUG)
    # maxBytes 5242880 will allow base file s3_single_entry.log to rotate 
    # after every 5MB. With a backupCount of 5, files will be generated 
    # like s3_single_entry.log, s3_single_entry.log.1 and up to s3.log.5.
    # The file being written to is always s3_single_entry.log.
    fhandler = logging.handlers.RotatingFileHandler(s3_single_entry_log_file, \
                                                    mode='a',\
                                                    maxBytes = 5242880,\
                                                    backupCount = 5,\
                                                    encoding=None, delay=False)
    fhandler.setLevel(logging.DEBUG)

    # create console handler with a higher log level
    chandler = logging.StreamHandler(sys.stdout)
    chandler.setLevel(logging.DEBUG)

    formatter = logging.Formatter(s3_single_entry_log_format)

    # create formatter and add it to the handlers
    fhandler.setFormatter(formatter)
    chandler.setFormatter(formatter)

    # add the handlers to the logger
    logger.addHandler(fhandler)
    logger.addHandler(chandler)

    return logger

def create_logger_directory(s3_single_entry_log_directory):
    """Create log directory if not exsists."""
    _logger_directory = os.path.join(s3_single_entry_log_directory)
    if not os.path.isdir(_logger_directory):
        try:
            os.makedirs(_logger_directory)
        except OSError as e:
            if e.errno == errno.EEXIST:
                pass
            else:
                raise Exception("Startup Logger Could not be created")

def get_fid(fid_path):
    """ Get FID from FID file"""
    Fid = None
    with open(fid_path) as file1:
      Lines = file1.readlines()
      for line in Lines:
        if(line.startswith('MOTR_PROCESS_FID')):
          Fid = line.split("=")[1]
          break
    if not Fid:
      raise Exception("FID is not present in Fid path")
    Fid = Fid.strip("\n")
    Fid = Fid.strip("''")

    return Fid

def configure_log_rotate(key_to_read: str, cron_dir: str):
    s3_logrotate_config_file = s3_confkeys_store.get_config(key_to_read)
    logger.info(f"s3_logrotate_config_file : {s3_logrotate_config_file}")
    os.makedirs(os.path.dirname(cron_dir), exist_ok=True)
    shutil.copy(s3_logrotate_config_file, cron_dir)
    logger.info("logrotate config file copied successfully to cron directory")

def create_symbolic_link(src_path: str, dst_path: str):
  """create symbolic link."""
  logger.info(f"symbolic link source path: {src_path}")
  logger.info(f"symbolic link destination path: {dst_path}")
  if os.path.exists(dst_path):
    logger.info(f"symbolic link is already present")
    os.unlink(dst_path)
    logger.info("symbolic link is unlinked")
  os.symlink(src_path, dst_path)
  logger.info(f"symbolic link created successfully")

def update_haproxy_log_rotate_file(filename: str,
                      content_to_search: str,
                      content_to_replace: str):
  """find and replace the haproxy log path and delete unwanted lines from logrotate file."""
  logger.info(f"log file path to search: {content_to_search}")
  logger.info(f"correct log file path prefix to replace: {content_to_replace}")
  # removing unwanted entries from logrotate file
  with open(filename) as f:
    newText=f.read().replace(content_to_search, content_to_replace)
  logger.debug(f"Updating haproxy log rotate file : {filename}")
  with open(filename, "w") as f:
    lines = newText.split("\n")
    for line in lines:
        entry=line.strip()
        if (entry.startswith("postrotate"))  \
          or  (entry.startswith("/bin/kill -HUP")) \
          or (entry.startswith("endscript")) \
          or (entry.startswith("rotate")) \
          or (entry.endswith("/haproxy-status.log")):
          # ignoring unwanted entries
          logger.debug(f"ignoring the entry : {entry}")
          pass
        elif (entry.startswith("}")):
          f.write("    rotate 5" +'\n')
          f.write("    size 5M" +'\n')
          f.write(line)
        else:
          f.write(line+'\n')
    logger.debug(f"Updated haproxy log rotate file successfully..")

  logger.info(f"haproxy log rotate file updation completed successfully :  {filename}.")

if __name__ == '__main__':
  sys.exit(main())
