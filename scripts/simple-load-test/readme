#
# Copyright (c) 2020 Seagate Technology LLC and/or its Affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# For any questions about this software or licensing,
# please email opensource@seagate.com or cortx-questions@seagate.com.
#


Scripts for simplified load tests.

In general, for performance benchmarking it is recommended to use cosbench.

Thing with cosbench is, it requires _a lot_ of system resources to run.  To max
out single s3 server node you might need e.g. 3 cosbench client nodes.  In case
there is not enough client nodes, or it is not possible to install cosbench in
your environment, you can use these simplified workloads defined in scripts in
this folder.

Note that these scripts might not yield max possible throughput, even if amount
of available resources is enough. Some how, cosbench is better and squeezing
out the maximum.


## rwrite.alias

General usage:

  $ source rwrite.alias
  $ bucket=s3://t7ko files=16 workers=4 size=256 rwrite
  $ bucket=s3://t7ko files=16 workers=4 size=256 rwrite_multibucket
  $ bucket=s3://t7ko files=10 rwrite_ttfb
  $ bucket=s3://t7ko files=10 rread_ttfb

Alias file defines shell (bash) aliases/commands for testing.


`rwrite` command runs write workload.  Essentially, what it does is, it runs
multiple parallel `aws s3 cp` commands to upload files to a given s3 bucket.
Files are generated on the fly (in ram) using `dd` command, by reading from
/dev/zero.

Parameters for the command are specified in the form of key=value pairs in
front of the rwrite command.  For example, to specify bucket name, use
`bucket=s3://name`.  Full list of parameters:

* `bucket`  -- full bucket name with s3 prefix and no trailing slash.
* `workers` -- number of cp commands to run in parallel.
* `size`    -- size of each file (in megabytes).
* `files`   -- number of files to send in total before the test ends.


`rwrite_multibucket` is very much like `rwrite`, except that it creates
individual bucket for each file to be uploaded (this test was added to see if
such separation will help with KVS locks).


General note: both commands use aws s3 cli to send data.  An attempt to replace
with s3cmd was made, but aws cli shows better performance.  To install and
configure aws cli, follow steps outlined in the following document:

     https://docs.google.com/document/d/1uuZ3LAfpiCyt4iitpAUdvc05cwr4hp4kMcbtWLG7Xd8/edit?ts=5d43fce9#



`rwrite_ttfb` and
`rread_ttfb`  commands are an attempt to measure TTFB (time to first byte for
write and read operations respectively).  To simplify things, we simply create
(for write test) or read (for read test) objects of 1 byte, sequentially, one
by one.  If you want to measure TTFB in system under load -- you'll need to
generate this load separately, and then, while your load is running, launch
this command to see how TTFB is affected.

* `bucket`  -- full bucket name with s3 prefix and no trailing slash.
* `files`   -- number of iteratsion in the test.
